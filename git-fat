#!/usr/bin/env python
# -*- mode:python -*-

from __future__ import print_function, with_statement

import sys
import hashlib
import tempfile
import os
import subprocess as sub
import itertools
import threading
import time
import collections
from multiprocessing import Process, Manager

try:
    from subprocess import check_output
    del check_output  # noqa

except ImportError:

    def backport_check_output(*popenargs, **kwargs):
        '''
        Run command with arguments and return its output as a byte string.
        Backported from Python 2.7 as it's implemented as pure python on stdlib.

        >> check_output(['/usr/bin/python', '--version'])
        Python 2.6.2
        '''
        process = sub.Popen(stdout=sub.PIPE, *popenargs, **kwargs)
        output, unused_err = process.communicate()
        retcode = process.poll()
        if retcode:
            cmd = kwargs.get("args")
            if cmd is None:
                cmd = popenargs[0]
            error = sub.CalledProcessError(retcode, cmd)
            error.output = output
            raise error
        return output

    sub.check_output = backport_check_output

BLOCK_SIZE = 4096


def git(cliargs, *args, **kwargs):
    ''' Calls git commands with Popen arguments '''
    return sub.Popen(['git'] + cliargs, *args, **kwargs)


def verbose_stderr(*args, **kwargs):
    return print(*args, file=sys.stderr, **kwargs)


def verbose_ignore(*args, **kwargs):
    pass


def mkdir_p(path):
    import errno
    try:
        os.makedirs(path)
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise


def umask():
    '''
    Get umask without changing it.
    '''
    old = os.umask(0)
    os.umask(old)
    return old


def readblocks(stream):
    '''
    Reads BLOCK_SIZE from stream and yields it
    '''
    while True:
        data = stream.read(BLOCK_SIZE)
        if not data:
            break
        yield data



def cat_iter(initer, outstream):
    for block in initer:
        outstream.write(block)


def cat(instream, outstream):
    return cat_iter(readblocks(instream), outstream)


def gitconfig_get(name, file=None):
    args = ['config', '--get']
    if file is not None:
        args += ['--file', file]
    args.append(name)
    p = git(args, stdout=sub.PIPE)
    output = p.communicate()[0].strip()
    if p.returncode != 0:
        return None
    else:
        return output


def gitconfig_set(name, value, file=None):
    args = ['git', 'config']
    if file is not None:
        args += ['--file', file]
    args += [name, value]
    sub.check_call(args)


class GitFat(object):

    DecodeError = RuntimeError

    def __init__(self):

        self.verbose = verbose_stderr if os.environ.get('GIT_FAT_VERBOSE') else verbose_ignore
        self.gitroot = sub.check_output('git rev-parse --show-toplevel'.split()).strip()
        self.gitdir = sub.check_output('git rev-parse --git-dir'.split()).strip()
        self.objdir = os.path.join(self.gitdir, 'fat', 'objects')

        if os.environ.get('GIT_FAT_VERSION') == '1':
            self.encode = self.encode_v1
        else:
            self.encode = self.encode_v2

        def magiclen(enc):
            return len(enc(hashlib.sha1('dummy').hexdigest(), 5))

        self.magiclen = magiclen(self.encode)  # Current version
        self.magiclens = [magiclen(enc) for enc in [self.encode_v1, self.encode_v2]]  # All prior versions

    def _rsync_opts(self):
        cfgpath = os.path.join(self.gitroot, '.gitfat')
        remote = gitconfig_get('rsync.remote', file=cfgpath)
        ssh_port = gitconfig_get('rsync.sshport', file=cfgpath)
        ssh_user = gitconfig_get('rsync.sshuser', file=cfgpath)
        if remote is None:
            raise RuntimeError('No rsync.remote in %s' % cfgpath)
        return remote, ssh_port, ssh_user

    def _rsync(self, push):
        (remote, ssh_port, ssh_user) = self._rsync_opts()
        if push:
            self.verbose('Pushing to %s' % (remote))
        else:
            self.verbose('Pulling from %s' % (remote))

        cmd = ['rsync', '--progress', '--ignore-existing', '--from0', '--files-from=-']
        rshopts = ''
        if ssh_user:
            rshopts += ' -l ' + ssh_user
        if ssh_port:
            rshopts += ' -p ' + ssh_port
        if rshopts:
            cmd.append('--rsh=ssh' + rshopts)
        if push:
            cmd += [self.objdir + '/', remote + '/']
        else:
            cmd += [remote + '/', self.objdir + '/']
        return cmd

    def encode_v1(self, digest, bytes):
        'Produce legacy representation of file to be stored in repository.'
        return '#$# git-fat %s\n' % (digest, )

    def encode_v2(self, digest, bytes):
        'Produce representation of file to be stored in repository. 20 characters can hold 64-bit integers.'
        return '#$# git-fat %s %20d\n' % (digest, bytes)

    def _decode(self, stream):
        '''
        Returns iterator and True if stream is git-fat object
        '''
        cookie = '#$# git-fat '
        stream_iter = readblocks(stream)
        # Read block for check
        block = next(stream_iter)

        def prepend(blk, iterator):
            yield blk
            for i in iterator:
                yield i

        # Put block back
        ret = prepend(block, stream_iter)
        if block.startswith(cookie):
            assert(len(block) == self.magiclen)  # Sanity check
            return ret, True
        return ret, False

    def _get_digest(self, stream):
        '''
        Returns digest if stream is fatfile placeholder or '' if not
        '''
        stream, fatfile = self._decode(stream)
        if fatfile:
            block = next(stream)  # read the first block
            digest = block.split()[2]
            return digest
        return ''

    def _cached_objects(self):
        '''
        Returns a set of all the cached objects
        '''
        return set(os.listdir(self.objdir))

    def _referenced_objects(self, rev=None, all=False):
        rev = rev or 'HEAD'
        # override rev if all is set
        if all:
            rev = '--all'

        objs_dict = self._managed_files(rev=rev)
        return set(objs_dict.keys())

    def _managed_files(self, rev=None, working=False):
        '''
        Finds managed files in the specified revision
        '''
        # TODO: Liberally comment this code

        rev = rev or 'HEAD'
        args = ['--no-walk', rev] if working else [rev]

        revlist = git('rev-list --objects'.split() + args, stdout=sub.PIPE)
        awk = sub.Popen(['awk', '{print $1}'], stdin=revlist.stdout, stdout=sub.PIPE)
        catfile = git('cat-file --batch-check'.split(), stdin=awk.stdout, stdout=sub.PIPE)

        managed = {}
        for line in catfile.stdout:
            objhash, objtype, size = line.split()
            if objtype == 'blob' and int(size) in self.magiclens:
                readfile = git(['cat-file', '-p', objhash], stdout=sub.PIPE)
                digest = self._get_digest(readfile.stdout)
                if digest:
                    managed[objhash] = digest

        catfile.wait()

        filedict = {}
        revlist2 = git('rev-list --objects'.split() + args, stdout=sub.PIPE)
        for line in revlist2.stdout:
            hashobj = line.split()
            if len(hashobj) == 2:
                if hashobj[0] in managed.keys():
                    filedict[hashobj[0]] = hashobj[1]
        revlist2.wait()

        ret = dict((j, filedict[i]) for i,j in managed.iteritems())
        return ret

    def _orphan_files(self, patterns=[]):
        '''
        generator for placeholders in working tree that match pattern
        '''
        # Null-terminated for proper file name handling
        for fname in sub.check_output(['git', 'ls-files', '-z'] + patterns).split('\x00')[:-1]:
            stat = os.lstat(fname)
            if stat.st_size != self.magiclen or os.path.islink(fname):
                continue
            with open(fname) as f:
                digest = self._get_digest(f)
                if digest:
                    yield (digest, fname)

    def list_files(self, rev=None, working=False):
        managed = self._managed_files(rev=rev, working=working)
        for f in managed.keys():
            print(f, managed.get(f))

    def checkout(self, show_orphans=False):
        '''
        Update any stale files in the present working tree
        '''
        for digest, fname in self._orphan_files():
            objpath = os.path.join(self.objdir, digest)
            if os.access(objpath, os.R_OK):
                print('Restoring %s -> %s' % (digest, fname))
                # The output of our smudge filter depends on the existence of
                # the file in .git/fat/objects, but git caches the file stat
                # from the previous time the file was smudged, therefore it
                # won't try to re-smudge. I don't know a git command that
                # specifically invalidates that cache, but touching the file
                # also does the trick.
                os.utime(fname, None)
                # This re-smudge is essentially a copy that restores permissions.
                sub.check_call(['git', 'checkout-index', '--index', '--force', fname])
            elif show_orphans:
                print('Data unavailable: %s %s' % (digest, fname))

    def can_add_file(self, filename):
        '''
        Checks to see if the current file exists in the local repo before filter-clean
        This method prevents fat from hijacking glob matches that are old
        '''
        # TODO: Add a method for explicitly adding glob files
        # If the file doesn't exist in the previous revision, add it
        showfile = git('show HEAD:{}'.format(filename).split(), stdout=sub.PIPE, stderr=sub.PIPE)
        if (showfile.wait()):
            return True
        # If it is already tracked, add it
        stream, is_fatfile = self._decode(showfile.stdout)
        return is_fatfile

    def checkconfig(self):
        '''
        Returns true if git-fat is already configured
        '''
        return gitconfig_get('filter.fat.clean') and gitconfig_get('filter.fat.smudge')

    def filter_smudge(self, instream, outstream):
        '''
        The smudge filter runs whenever a file is being checked out into the working copy of the tree
        instream is sys.stdin and outstream is sys.stdout when it is called by git
        '''
        stream, fatfile = self._decode(instream)
        if fatfile:
            block = next(stream)  # read the first block
            digest = block.split()[2]
            objfile = os.path.join(self.objdir, digest)
            try:
                cat(open(objfile), outstream)
                self.verbose('git-fat filter-smudge: restoring from %s' % objfile)
            except IOError:
                self.verbose('git-fat filter-smudge: fat object not found in cache %s' % objfile)
                outstream.write(block)
        else:
            self.verbose('git-fat filter-smudge: not a managed file')
            cat_iter(stream, sys.stdout)

    def filter_clean(self, instream, ostream_toindex):
        '''
        The clean filter runs when a file is added to the index. It gets the "smudged" (working copy)
        version of the file on stdin and produces the "clean" (repository) version on stdout.
        '''

        self.verbose("In filter clean for file {}".format(sys.argv[2]))
        hasher = hashlib.new('sha1')
        bytes = 0
        fd, tmpname = tempfile.mkstemp(dir=self.objdir)
        cached = False

        try:
            blockiter, is_clean = self._decode(instream)

            # Changes to True when file exists in .git/fat/objects
            # Open the temporary file for writing
            ostream = ostream_toindex

            # if it's not a git-fat placeholder file, cache it
            if not is_clean:
                ostream = os.fdopen(fd, 'w')

            for block in blockiter:
                # Add the block to be hashed
                hasher.update(block)
                bytes += len(block)
                ostream.write(block)

            ostream.flush()
            digest = hasher.hexdigest()
            objfile = os.path.join(self.objdir, digest)

            # Create placeholder for the file
            if not is_clean:
                # Close temporary file
                ostream.close()
                if os.path.exists(objfile):
                    self.verbose('git-fat filter-clean: cached file already exists %s' % objfile)
                    os.remove(tmpname)
                else:
                    # Set permissions for the new file using the current umask
                    os.chmod(tmpname, int('444', 8) & ~umask())
                    os.rename(tmpname, objfile)
                    self.verbose('git-fat filter-clean: caching to %s' % objfile)
                cached = True
                # Write placeholder to index
                ostream_toindex.write(self.encode(digest, bytes))
        finally:
            if not cached:
                os.remove(tmpname)

    def pull(self, rev=None, pattern=None):
        '''
        Pull anything that I have referenced, but not stored
        '''
        revision = rev or 'HEAD'

        cached_objs = self._cached_objects()
        if pattern:
            # filter the working tree by a pattern
            files = set(self._orphan_files(patterns=[pattern])) - cached_objs
        else:
            # default pull any object referenced but not stored
            files = self._referenced_objects(rev=revision) - cached_objs

        if files:
            print("Pulling: ", list(files))
            cmd = self._rsync(push=False)
            self.verbose('Executing: %s' % ' '.join(cmd))
            p = sub.Popen(cmd, stdin=sub.PIPE)
            p.communicate(input='\x00'.join(files))
        else:
            print("You've got everything! d(^_^)b")

        self.checkout()

    def push(self, pushall=False):
        '''
        Push anything that I have stored and referenced (rsync doesn't push if exists on remote)
        '''
        # Default to push only those objects referenced by current HEAD
        # (includes history). Finer-grained pushing would be useful.
        files = self._referenced_objects(all=pushall) & self._cached_objects()
        rsync = self._rsync(push=True)
        self.verbose('Executing: %s' % ' '.join(cmd))
        p = sub.Popen(rsync, stdin=sub.PIPE)
        p.communicate(input='\x00'.join(files))

    def init(self):
        '''
        Create cache directory and setup filters in .git/config
        '''
        mkdir_p(self.objdir)
        gitconfig_set('filter.fat.clean', 'git-fat filter-clean %f')
        gitconfig_set('filter.fat.smudge', 'git-fat filter-smudge %f')
        print('Initialized git fat')

    def gc(self):
        '''
        Remove any objects that aren't referenced in the tree
        '''
        garbage = self._cached_objects() - self._referenced_objects()
        print('Unreferenced objects to remove: %d' % len(garbage))
        for obj in garbage:
            fname = os.path.join(self.objdir, obj)
            print('%10d %s' % (os.stat(fname).st_size, obj))
            os.remove(fname)

    def status(self, args):
        '''
        Show orphan (in tree, but not in cache) and garbage (in cache, but not in tree) objects, if any.
        '''
        catalog = self._cached_objects()
        all = '--all' in args
        referenced = self._referenced_objects(all=all)
        garbage = catalog - referenced
        orphans = referenced - catalog
        if all:
            for obj in referenced:
                print(obj)
        if orphans:
            print('Orphan objects:')
            for orph in orphans:
                print('    ' + orph)
        if garbage:
            print('Garbage objects:')
            for g in garbage:
                print('    ' + g)

if __name__ == '__main__':
    fat = GitFat()
    cmd = sys.argv[1] if len(sys.argv) > 1 else ''
    # If we're not configured and they're not calling init
    if not fat.checkconfig() and not cmd == 'init':
        sys.stderr.write("git-fat not configured. First run: git fat init\n")
        sys.exit(1)
    if cmd == 'init':
        fat.init()
    elif cmd == 'filter-clean':
        if fat.can_add_file(sys.argv[2]):
            fat.filter_clean(sys.stdin, sys.stdout)
        else:
            verbose_stderr(
                "Not adding: {}\n".format(sys.argv[2]) +
                "It is not a new file and is not managed by git-fat"
            )
            cat(sys.stdin, sys.stdout)
    elif cmd == 'filter-smudge':
        verbose_stderr(sys.argv[2])
        fat.filter_smudge(sys.stdin, sys.stdout)
    elif cmd == 'status':
        fat.status(sys.argv[2:])
    elif cmd == 'push':
        fat.push()
    elif cmd == 'pull':
        if len(sys.argv) > 2:
            fat.pull(rev=sys.argv[2])
        else:
            fat.pull()
    elif cmd == 'gc':
        fat.gc()
    elif cmd == 'checkout':
        fat.checkout(show_orphans=True)
    elif cmd == 'list':
        fat.list_files(working=True)
    else:
        print('Usage: git fat (init|status|list|push|pull|gc|checkout)', file=sys.stderr)
